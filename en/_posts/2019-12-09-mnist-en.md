---
layout: post
lang: en
ghcode: backpropagationForMNISTdataset
lang-ref: mnist
title: "Multilayer Backpropagation to train and classify handwritten numbers (MNIST)"
date: 2019-12-18 12:00:00 -0300
description: Backpropagation algorithm written in C and Matlab to train and classify handwritten numbers (0-9) from MNIST dataset.
img: mnist_exemplo.png # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [Artificial Intelligence,C,Matlab]
permalink: en-backpropagation-mnist
---
## MNIST
[MNIST](http://yann.lecun.com/exdb/mnist/) is a database with a total of **70,000 images** of handwritten numbers. The size of the database makes it ideal for artificial intelligence training and experimentation focused on image classification, so it was a great database for my first artificial intelligence project. Each image has a size of 28x28 pixels, each pixel has values ​​ranging from 0 to 255 as brightness values, and each image has a label to indicate the number represented by its image to aid the neural network learning process. The entire base is compressed and saved in binary format to save storage.

![Examples of MNIST base images.]({{site.baseurl}}/assets/img/mnist_sample.gif)

## Multilayer Backpropagation
Multilayer Backpropagation has that first half of its name due to the structuring of its perceptrons in the neural network, which contributes to both positive and negative ways of its functioning. The neural network is said to have multiple layers because all the calculations and results that go through the perceptrons can be divided into steps or phases; In each of these steps, a certain number of perceptrons architechtured by the programmer receives information from the previous layer, perform calculations, and pass them on to a next step in the learning process. **The multilayer system has the advantage of allowing the neural network to classify data into three or more outputs**, while its disadvantage is the difficulty of adapting its perceptrons to better recognize input data to their respective output values ​​due to the high connectivity between perceptrons.

The term backpropagation comes from the form in which the network is transformed to create or improve the links between input and output information. First, in the forward phase, the input data is processed by the first layer's perceptrons, as argument of a differentiable function, resulting in new values to then be processed through the next perceptron layer, going through another differentiable function again, and so on until all layers evaluates data and the final result is processed. In the second phase, called the backward phase, the error function quantify the error associated with the neural network's classification. That resulting error goes back through new functions (derivatives of the forward phase functions) so that the error can be backpropagated throughout the neural network to modify its weights and bias of each perceptron, improving the network classification process. To obtain the best possible network, the error should be minimal. Knowing that the error is a differentiable function from the input data, we can find its minimum value by calculating its gradient.

![Example of backpropagation. Source: NNDesign - Hagan et. al.]({{site.baseurl}}/assets/img/mnist_backpropagationexemplo.png)

## Properties choice for the neural network
Several adjustments can be made to the neural network to improve its learning process or change the training duration. Because this neural network was built with C language, many features aren't adjusted due to the difficulty to perform simple tasks with this programming language such as evaluating matrixes multiplications and allocating memory (these tasks are much harder while using C when you're working with more than 54 million input values ​​and 266,000 network parameters). Therefore, the only adjustments made to the network properties were on the number of neurons used in each layer (as well as the number of layers used) and the choice of the best learning rate value.

For the first property, care must be taken with both the excess and the lack of neurons in the network; A good choice about the numbers of neurons and layers can result in a fast and efficient neural network. In this project, three layers were used, with 300 neurons in the first layer, 100 in the second and 10 neurons in the output layer. With the neural network architecture defined, several tests were performed testing different values ​​for the learning rate and the results are presented in the table (or image) below; values ​​outside the learning rate range showed higher percentage errors. For the tests, only 6000 images for training and 4000 images for neural network tests were used. The table below shows different percentage values ​​for the error associated with each learning rate.

![Learning rate values vs. Error.]({{site.baseurl}}/assets/img/mnist_learningrate.png)

The network accuracy with the chosen architecture was approximately 87.64%. Later, I decided to use the same algorithm but this time using Matlab with also all available samples from the dataset; in Matlab, I had easier to allocate data and work with all images at the same time. **The result of the increase of images in the test sample was a neural network with 93.8% of precision in numbers classification.** Interested in the origin of errors in the classification of the network even after a significant increase in the sample used for training, I decided to generate some of the images along with the values sorted by the network as well as its correct label, shown in the image below. **One can notice that the numbers can easily confuse even human readers.** Therefore, in this case, we may think that the neural network has a good enough accuracy rate when the written number is not such an unreadable scribble.

![Example of bad guesses from the network.]({{site.baseurl}}/assets/img/mnist_erros.png)