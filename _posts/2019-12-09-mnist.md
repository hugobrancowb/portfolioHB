---
layout: post
lang: pt
ghcode: backpropagationForMNISTdataset
lang-ref: mnist
title: "Reconhecimento de números utilizando Multilayer Backpropagation"
date: 2019-12-18 12:00:00 -0300
description: Treino e validação de rede neural capaz de reconhecer algarismos manuscritos utlizando inteligência artificial.
img: mnist_exemplo.png # Add image post (optional)
fig-caption: Exemplos de imagens da base MNIST. Fonte - Wikipedia. # Add figcaption (optional)
tags: [Inteligência Artificial,C,Matlab]
permalink: backpropagation-mnist
---
## MNIST
[MNIST](http://yann.lecun.com/exdb/mnist/) é uma database com o total de **70.000 imagens** de algarismos escritos à mão. O tamanho da database a torna ideal para treinamento e experimentação em inteligência artificial voltada para classificação de imagens, portanto foi uma ótima base de dados para meu primeiro projeto de inteligência artificial. Cada imagem da base tem dimensão 28x28 pixels, cada pixel tem valores que variam entre 0 e 255 para dar forma à imagem, e cada imagem possui um rótulo para informar qual o número representado pela respectiva imagem a fim de facilitar o processo de aprendizado das redes neurais. Toda a base é comprimida e salva em formato binário para facilitar armazenamento e acesso.

![Exemplos de imagens da base MNIST. Fonte: Wikipedia.]({{site.baseurl}}/assets/img/mnist_sample.gif)

## Multilayer Backpropagation

Na *Multilayer Backpropagation*, a primeira metade de seu nome resulta da estruturação de seus perceptrons na rede neural, que tem como consequência os pontos tanto positivos quanto negativos de seu funcionamento. Diz-se que a rede neural possui múltiplas camadas pois o processamento das informações através dos perceptrons pode ser dividido em etapas; em cada uma destas etapas, um certo número de perceptrons escolhido pelo programador recebe informações da camada anterior, executa cálculos com as mesmas e as repassa para uma próxima etapa do processo de aprendizado. **O sistema de múltiplas camadas tem como vantagem permitir que a rede neural construída possa distinguir três ou mais padrões de dados de entrada**, ao passo que sua desvantagem, também causada pela estruturação multi camadas, é a dificuldade de transformar seus perceptrons a fim de relacionarem melhor os dados de entrada a seus respectivos valores de saída devido a alta conectividade entre os perceptrons.

O termo *backpropagation* tem origem na forma da qual a rede se transforma a fim de criar ou melhorar a relação entre os dados de entrada e a saída da informação processada. Primeiro, na fase de progresso (termo livremente traduzido à partir da expressão *foward phase*), as informações de entrada são captadas por todos perceptrons da primeira camada, processadas por uma função diferenciável em cada um deles resultando num subproduto para, então, ser processador pela camada de perceptrons seguinte, passando novamente por outra função diferenciável, e assim por diante até que todas camadas recebam os dados de entrada e o resultado final seja processado. Na segunda fase, chamada fase de retorno (ou *backward phase*), um sinal de erro é gerado ao comparar o resultado processado com o resultado esperado. O sinal passa por novas funções (derivadas das funções da fase de progresso) para que uma correção possa ser retropropagada (*backpropagated*) por toda rede neural com o objetivo de modificar os pesos de cada perceptron, melhorando o processo de classificação da rede. Para obter a melhor rede neural possível, o erro final deve ser mínimo. Sabendo que o erro é uma função diferenciável dos dados de entrada, podemos encontrar seu valor mínimo à partir do cálculo de seu gradiente.

![Exemplo de backpropagation. Fonte: NNDesign - Hagan et. al.]({{site.baseurl}}/assets/img/mnist_backpropagationexemplo.png)

## Propriedades da rede neural utilizada
Existem diversos ajustes que podem ser feitos na rede neural a fim de melhorar seu processo de aprendizado ou alterar o tempo de processamento de acordo com a necessidade. Como essa rede neural foi construída utilizando C, muitos recursos deixaram de ser explorados devido à dificuldade de se trabalhar com a linguagem para realizar tarefas simples como multiplicação de matrizes e alocação de memória (essas tarefas tornam-se muito mais complexas utilizando C quando se trabalha com, no total, mais de 54 milhões valores de entrada e mais de 266 mil parâmetros na rede). Portanto, os unicos ajustes feitos nas propriedades da rede foram na escolha da quantidade de neurons utilizados em cada camada (assim como a quantidade de camadas utilizadas) e na escolha da melhor taxa de aprendizado (*learning rate*).

Para a primeira propriedade, é necessário ter cuidado tanto com o excesso quanto com a falta de neurons na rede; uma boa escolha na quantidade de neurons e camadas resulta numa rede neural rápida e eficiente. Nesse projeto, utilizou-se três camadas, com 300 na primeira, 100 na segunda e 10 neurons na camada de saída. Com a arquitetura da rede neural definida, vários testes foram realizados testando diferentes valores para a *learning rate* e os resultados apresentam-se na imagem abaixo; valores fora do intervalo trabalhado apresentaram erro percentual muito alto. Para a realização dos testes, foram utilizadas apenas 6000 imagens para treino e 4000 para teste da rede neural. A tabela abaixo apresenta diferentes valores percentuais para o erro associado a cada *learning rate*.

![Erros associados a suas respectivas taxas de aprendizado]({{site.baseurl}}/assets/img/mnist_learningrate.png)

Portanto a precisão da rede, nesse momento, foi de aproximadamente 87,64%. Mais tarde decidi o mesmo algoritmo porém em Matlab e utilizando toda base de dados disponível; no Matlab, tive maior facilidade para alocar dados e trabalhar com todas imagens ao mesmo tempo. **O resultado do aumento da amostra de testes foi uma rede neural com precisão de 93,8% no reconhecimento dos algarismos.** Curioso com a causa dos erros na classificação da rede mesmo após um aumento significativo da amostra utilizada para treino, decidi gerar algumas das imagens junto com os valores classificados pela rede assim como a legenda correta, exibidos na imagem abaixo. **É possível perceber que as imagens podem facilmente causar confusão até mesmo em humanos**. Portanto, nesse caso, talvez possamos julgar que a rede neural tem uma taxa de acertos boa o suficiente quando o número escrito não é um garrancho ilegível.

![Exemplo das imagens que nossa rede classificou erroneamente.]({{site.baseurl}}/assets/img/mnist_erros.png)

